{"cells":[{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dataset = spark.table(\"bank\")\ncols = dataset.columns\n\n#display(dataset)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["categoricalColumns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol=\"y\", outputCol=\"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n\ndataset"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n  \npartialPipeline = Pipeline().setStages(stages)\npipelineModel = partialPipeline.fit(dataset)\npreppedDataDF = pipelineModel.transform(dataset)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Fit model to prepped data\nlrModel = LogisticRegression().fit(preppedDataDF)\n\n# ROC for training data\ndisplay(lrModel, preppedDataDF, \"ROC\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(lrModel, preppedDataDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = preppedDataDF.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\nprint(trainingData.count())\nprint(testData.count())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#salva o modelo\nlrModel.write().overwrite().save(\"modelo_lr1\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)\npredictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose columns\nselected = predictions.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\", \"age\", \"job\", \"education\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["evaluator.getMetricName()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#print(lr.explainParams())"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#salva o modelo\n#cvModel.save(\"modelo_cv\")\ncvModel.write().overwrite().save(\"modelo_cv\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["predictions = cvModel.transform(testData)\nprint(testData)\n# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["print('Model Intercept: ', cvModel.bestModel.intercept)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["weights = cvModel.bestModel.coefficients\nweights = [(float(w),) for w in weights]  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.mllib.evaluation import MulticlassMetrics\n\n# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\").rdd\nmetrics = MulticlassMetrics(selected)\nprint(metrics.confusionMatrix())\n\n# Overall statistics\nprecision = metrics.precision()\nrecall = metrics.recall()\nf1Score = metrics.fMeasure()\nprint(\"Summary Stats\")\nprint(\"Precision = %s\" % precision)\nprint(\"Recall = %s\" % recall)\nprint(\"F1 Score = %s\" % f1Score)\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegressionModel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.mllib.linalg import SparseVector\n\nmodelo = LogisticRegressionModel.load(\"modelo_lr1\")\n#print(trainingData.first())\n     \ndata1 = [(0, Vectors.sparse(42,[1,11,14,16,17,18,20,21,32,35,36,37,38,39,40],[1,1,1,1,1,1,1,1,1,58,2143,5,261,1,-1]), 24, \"blue-collar\", \"married\", \"secondary\", \"no\", 1470, \"yes\", \"no\", \"cellular\", 12, \"may\", 212, 1, -1, 0, \"unknown\")]\ndata2 = spark.createDataFrame(data1, [\"label\", \"features\"])\n \npredictions = modelo.transform(data2)\n\nprint(predictions)\n\nselected = predictions.select(\"label\", \"prediction\", \"rawPrediction\", \"probability\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"zam","notebookId":914871633331477},"nbformat":4,"nbformat_minor":0}
